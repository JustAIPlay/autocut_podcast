#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
import json
from pathlib import Path
from typing import Any, List, cast

# è®¾ç½® Hugging Face é•œåƒï¼Œè§£å†³å›½å†…ä¸‹è½½æ¨¡å‹å¤±è´¥çš„é—®é¢˜
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# é¢„åˆå§‹åŒ–ä»¥æ¶ˆé™¤æœªç»‘å®šè­¦å‘Š
WhisperModel = None
whisper = None

try:
    from faster_whisper import WhisperModel
    HAS_FASTER = True
except ImportError:
    try:
        import whisper
        HAS_FASTER = False
    except ImportError:
        print("è¯·å…ˆå®‰è£… whisper: pip install openai-whisper æˆ– pip install faster-whisper")
        sys.exit(1)

def format_timestamp(seconds: float):
    """å°†ç§’æ•°è½¬æ¢ä¸º SRT æ—¶é—´æ ¼å¼"""
    td_hours = int(seconds // 3600)
    td_mins = int((seconds % 3600) // 60)
    td_secs = int(seconds % 60)
    td_millis = int((seconds - int(seconds)) * 1000)
    return f"{td_hours:02}:{td_mins:02}:{td_secs:02},{td_millis:03}"

def split_subtitle_text(text, max_len=14):
    """
    è¯­ä¹‰ä¼˜å…ˆçš„åˆ‡åˆ†é€»è¾‘ï¼š
    1. ä¼˜å…ˆå¯»æ‰¾æ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ‡åˆ†ã€‚
    2. å¦‚æœæ²¡æœ‰æ ‡ç‚¹ï¼Œä½¿ç”¨åˆ†è¯ä¿è¯è¯è¯­å®Œæ•´ã€‚
    3. é¿å…åˆ‡åˆ†åäº§ç”ŸæçŸ­çš„â€œå°¾å·´â€ã€‚
    """
    if not text or len(text) <= max_len:
        return [text] if text else []

    # å®šä¹‰æ–­å¥æ ‡ç‚¹
    punctuations = ["ï¼Œ", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼š", ",", ".", "!", "?", ";", ":"]
    
    # å¦‚æœæ–‡æœ¬ä¸­æœ‰æ ‡ç‚¹ï¼Œå°è¯•åœ¨æ ‡ç‚¹å¤„åˆ‡åˆ†
    split_pos = -1
    for i in range(len(text) - 1, 0, -1):
        # å¯»æ‰¾å¤„äºâ€œé»„é‡‘åˆ‡åˆ†åŒºâ€ï¼ˆ60%-95%é•¿åº¦å¤„ï¼‰çš„æ ‡ç‚¹
        if text[i] in punctuations:
            # å¦‚æœæ ‡ç‚¹åçš„å†…å®¹å¤ªçŸ­ï¼ˆå°‘äº3ä¸ªå­—ï¼‰ï¼Œç»§ç»­å¾€å‰æ‰¾
            if len(text) - i - 1 < 3:
                continue
            # å¦‚æœæ ‡ç‚¹å‰çš„å†…å®¹åœ¨åˆç†èŒƒå›´å†…ï¼ˆä¸è¶…è¿‡ max_len * 1.5ï¼‰
            if i + 1 <= max_len * 1.5:
                split_pos = i + 1
                break

    if split_pos != -1:
        part1 = text[:split_pos]
        part2 = text[split_pos:]
        # é€’å½’å¤„ç†å‰©ä½™éƒ¨åˆ†
        return [part1] + split_subtitle_text(part2, max_len)

    # å¦‚æœæ²¡æœ‰åˆé€‚çš„æ ‡ç‚¹ï¼Œå›é€€åˆ°è¯æ³•åˆ‡åˆ†
    try:
        import jieba
        import logging
        jieba.setLogLevel(logging.INFO)
        words = list(jieba.cut(text))
    except ImportError:
        import re
        words = re.findall(r'[\u4e00-\u9fff]|[a-zA-Z0-9]+|[^\u4e00-\u9fff\a-zA-Z0-9]', text)

    # å¯»æ‰¾æœ€æ¥è¿‘ä¸­é—´çš„è¯è¾¹ç•Œ
    mid = len(text) // 2
    curr_len = 0
    best_split = -1
    min_diff = len(text)
    
    for i, word in enumerate(words):
        curr_len += len(word)
        diff = abs(curr_len - mid)
        if diff < min_diff:
            # é¿å¼€ç¦å¿Œè¯å¼€å¤´
            forbidden = ["çš„", "äº†", "ç€", "å„¿", "æ—¶"]
            if i + 1 < len(words) and words[i+1] not in forbidden:
                min_diff = diff
                best_split = curr_len
    
    if best_split != -1 and best_split < len(text):
        part1 = text[:best_split]
        part2 = text[best_split:]
        return [part1] + split_subtitle_text(part2, max_len)
    
    return [text]

def transcribe(script_id):
    """æ ¹æ®è„šæœ¬ ID è¿›è¡ŒéŸ³é¢‘è½¬å½•"""
    global WhisperModel, whisper
    
    # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„ç®¡ç†
    from utils import get_script_paths
    paths = get_script_paths(script_id)
    audio_path = paths["audio"]
    copy_dir = paths["copy_whisper"].parent
    caption_dir = paths["caption_whisper_srt"].parent
    
    if not audio_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        return False

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    caption_dir.mkdir(parents=True, exist_ok=True)
    copy_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸš€ æ­£åœ¨è½¬å½•é¡¹ç›®: {script_id}")
    
    full_text = ""
    segments: List[Any] = []

    if HAS_FASTER and WhisperModel is not None:
        print(f"ğŸš€ ä½¿ç”¨ faster-whisper (GPU åŠ é€Ÿæ¨¡å¼) è½¬å½•...")
        try:
            # å°è¯•ä½¿ç”¨ GPU (CUDA) å’Œ float16 ç²¾åº¦
            # å‡çº§ä¸º medium æ¨¡å‹ï¼Œå¹³è¡¡é€Ÿåº¦ä¸æé«˜çš„å‡†ç¡®ç‡
            model = WhisperModel("medium", device="cuda", compute_type="float16")
        except Exception as e:
            print(f"âš ï¸ GPU åŠ é€Ÿå¯åŠ¨å¤±è´¥ï¼Œåˆ‡æ¢å› CPU æ¨¡å¼ã€‚é”™è¯¯: {e}")
            model = WhisperModel("medium", device="cpu", compute_type="int8")
        
        # ä¼˜åŒ– initial_promptï¼ŒåŒ…å«æ˜“é”™è¯çº æ­£
        result_iter, info = model.transcribe(
            str(audio_path), 
            beam_size=5, 
            language="zh", 
            initial_prompt="è¿™æ˜¯ä¸€æ®µå…³äºä¹”ä¸¹Â·è²åˆ©æ™®æ–¯åŒ»ç”Ÿåœ¨ä¸­å›½æ¨å¹¿è…¹è…”é•œæ‰‹æœ¯çš„åŒ»å­¦çºªå½•ç‰‡ã€‚å…³é”®è¯ï¼šå¡è¿›å…¬æ–‡åŒ…ã€å¹æ°”ã€è…¹è…”é•œã€æ‰‹æœ¯åˆ€ã€åå’ŒåŒ»é™¢ã€‚"
        )
        
        segments = []
        for s in result_iter:
            # å®æ—¶è¿›åº¦æŠ¥å‘Š
            print(f"  [{format_timestamp(s.start)}] {s.text}")
            segments.append(s)
            
        full_text = "".join([s.text for s in segments])
    elif whisper is not None:
        print("ğŸš€ ä½¿ç”¨ standard whisper è½¬å½•...")
        # æ ‡å‡†ç‰ˆä¹ŸåŒæ­¥å‡çº§
        model = whisper.load_model("medium")
        # æ ‡å‡† whisper åº“ä¹Ÿæ”¯æŒ language å’Œ initial_prompt
        result: Any = model.transcribe(
            str(audio_path), 
            language="zh", 
            initial_prompt="è¿™æ˜¯ä¸€æ®µå…³äºä¹”ä¸¹Â·è²åˆ©æ™®æ–¯åŒ»ç”Ÿåœ¨ä¸­å›½æ¨å¹¿è…¹è…”é•œæ‰‹æœ¯çš„åŒ»å­¦çºªå½•ç‰‡ã€‚å…³é”®è¯ï¼šå¡è¿›å…¬æ–‡åŒ…ã€å¹æ°”ã€è…¹è…”é•œã€æ‰‹æœ¯åˆ€ã€åå’ŒåŒ»é™¢ã€‚"
        )
        full_text = str(result.get("text", ""))
        segments = list(result.get("segments", []))
        for s in segments:
            print(f"  [{format_timestamp(float(s['start']))}] {s['text']}")

    # ä¿å­˜çº¯æ–‡æœ¬åˆ° copys ç›®å½•
    txt_path = paths["copy_whisper"]
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(full_text)
    
    # ä¿å­˜ SRT å­—å¹•å’Œ JSON
    srt_path = paths["caption_whisper_srt"]
    json_path = paths["caption_whisper_json"]
    
    raw_segments = []
    for segment in segments:
        if HAS_FASTER:
            raw_segments.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text.strip()
            })
        else:
            raw_segments.append({
                "start": segment['start'],
                "end": segment['end'],
                "text": segment['text'].strip()
            })

    # --- å¢åŠ ï¼šåˆ‡åˆ†è¿‡é•¿å­—å¹•é€»è¾‘ (æ¯æ®µä¸è¶…è¿‡ 9 ä¸ªå­—) ---
    MAX_CHARS = 9
    structured_segments = []
    idx = 1
    for seg in raw_segments:
        text = seg['text']
        parts = split_subtitle_text(text, MAX_CHARS)
        
        if len(parts) == 1:
            structured_segments.append({
                "id": idx,
                "start": round(float(seg['start']), 3),
                "end": round(float(seg['end']), 3),
                "text": parts[0]
            })
            idx += 1
        else:
            start = float(seg['start'])
            end = float(seg['end'])
            duration = end - start
            total_chars = len(text)
            
            current_start = start
            for part in parts:
                part_len = len(part)
                part_duration = (part_len / total_chars) * duration
                part_end = current_start + part_duration
                
                structured_segments.append({
                    "id": idx,
                    "start": round(current_start, 3),
                    "end": round(part_end, 3),
                    "text": part
                })
                idx += 1
                current_start = part_end
    # -----------------------------------------------

    # --- å¢åŠ ï¼šæœ€åæ¸…ç†ï¼Œåˆå¹¶çº¯æ ‡ç‚¹æ®µè½æˆ–ç¦å¿Œå¼€å¤´çš„æ®µè½ ---
    final_segments = []
    # æ‰©å±•ç¦å¿Œåˆ—è¡¨
    forbidden_at_start = [
        "çš„", "äº†", "ç€", "å„¿", "æ—¶", "ï¼‰", "ã€‘", "â€", "â€™", "ã€‹", "ï¼›", "ï¼š", "ï¼Œ", "ã€‚", "ï¼", "ï¼Ÿ", "ã€",
        ",", ".", "!", "?", ";", ":", ")", "]", "}", ">"
    ]
    for seg in structured_segments:
        # åªè¦å¤ªçŸ­ï¼ˆ<5å­—ï¼‰ï¼Œæˆ–è€…ä»¥æ ‡ç‚¹/åŠ©è¯å¼€å¤´ï¼Œæˆ–è€…å…¨æ˜¯æ ‡ç‚¹ï¼Œå°±åˆå¹¶
        if final_segments and (
            len(seg['text']) < 5 or 
            seg['text'][0] in forbidden_at_start or 
            all(c in forbidden_at_start for c in seg['text'])
        ):
            # åªè¦åˆå¹¶åä¸è¶…è¿‡16å­—
            if len(final_segments[-1]['text']) + len(seg['text']) <= 16:
                final_segments[-1]['end'] = seg['end']
                final_segments[-1]['text'] += seg['text']
            else:
                final_segments.append(seg)
        else:
            final_segments.append(seg)
    
    # é‡æ–°ç¼–å·å¹¶ç¡®ä¿æ—¶é—´æˆ³ç²¾åº¦
    for i, seg in enumerate(final_segments, 1):
        seg['id'] = i
        seg['start'] = round(seg['start'], 3)
        seg['end'] = round(seg['end'], 3)
    structured_segments = final_segments
    # ------------------------------------
    
    with open(srt_path, "w", encoding="utf-8") as f:
        for seg in structured_segments:
            # å†™å…¥ SRT
            start_str = format_timestamp(seg['start'])
            end_str = format_timestamp(seg['end'])
            f.write(f"{seg['id']}\n{start_str} --> {end_str}\n{seg['text']}\n\n")
    
    # ä¿å­˜ JSON (åŒ…å«å®Œæ•´çš„ç»“æ„åŒ–æ•°æ®ï¼Œä¾› compose_video.py ä½¿ç”¨)
    output_data = {
        "script_id": script_id,
        "full_text": full_text,
        "segments": structured_segments
    }
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è½¬å½•å®Œæˆï¼")
    print(f"   SRT: {srt_path}")
    print(f"   JSON: {json_path}")
    return True

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python transcribe_whisper.py <script_id>")
        sys.exit(1)
    
    transcribe(sys.argv[1])