#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-ForcedAligner æ—¶é—´æˆ³å¯¹é½è„šæœ¬
ä½¿ç”¨ Qwen3-ForcedAligner-0.6B ç”Ÿæˆè¯çº§æ—¶é—´æˆ³
æ”¯æŒé•¿éŸ³é¢‘è‡ªåŠ¨åˆ†æ®µå¤„ç†ï¼ˆè¶…è¿‡5åˆ†é’Ÿè‡ªåŠ¨åˆ‡åˆ†ï¼‰
"""
import os
import sys
import io
import json
import argparse
import tempfile
from pathlib import Path

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if hasattr(sys.stdout, 'buffer'):
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
if hasattr(sys.stderr, 'buffer'):
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from utils import get_script_paths, get_project_root

# å°è¯•å¯¼å…¥ä¾èµ–
try:
    from qwen_asr import Qwen3ForcedAligner
    HAS_ALIGNER = True
except ImportError:
    HAS_ALIGNER = False
    print("âš ï¸ è­¦å‘Š: æœªå®‰è£… qwen-asrï¼Œè¯·è¿è¡Œ: pip install qwen-asr")

try:
    import librosa
    import soundfile as sf
    import numpy as np
    HAS_AUDIO_LIBS = True
except ImportError:
    HAS_AUDIO_LIBS = False
    print("âš ï¸ è­¦å‘Š: æœªå®‰è£… librosa/soundfileï¼Œé•¿éŸ³é¢‘åˆ†æ®µåŠŸèƒ½ä¸å¯ç”¨")

# å¸¸é‡
MAX_AUDIO_DURATION = 270  # 4.5åˆ†é’Ÿï¼Œä¿ç•™ä½™é‡ï¼ˆå®˜æ–¹é™åˆ¶5åˆ†é’Ÿï¼‰
MIN_SILENCE_DURATION = 0.3  # æœ€å°é™éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
SILENCE_THRESHOLD = 0.02  # é™éŸ³é˜ˆå€¼


def extract_word_info(word_item) -> dict:
    """
    ä»è¯é¡¹ä¸­æå–ä¿¡æ¯ï¼Œæ”¯æŒå­—å…¸å’Œå¯¹è±¡ä¸¤ç§æ ¼å¼
    è¿”å›: {"word": str, "start": float, "end": float}
    """
    # å°è¯•å­—å…¸è®¿é—®
    if isinstance(word_item, dict):
        return {
            "word": word_item.get("word", word_item.get("text", "")),
            "start": word_item.get("start", word_item.get("start_time", 0.0)),
            "end": word_item.get("end", word_item.get("end_time", 0.0))
        }
    # å°è¯•å¯¹è±¡å±æ€§è®¿é—®
    else:
        word = getattr(word_item, "word", None) or getattr(word_item, "text", "")
        start = getattr(word_item, "start", None) or getattr(word_item, "start_time", 0.0)
        end = getattr(word_item, "end", None) or getattr(word_item, "end_time", 0.0)
        return {"word": str(word), "start": float(start), "end": float(end)}



def get_audio_duration(audio_path: Path) -> float:
    """è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
    if not HAS_AUDIO_LIBS:
        return 0.0
    y, sr = librosa.load(str(audio_path), sr=None)
    return len(y) / sr


def find_silence_points(audio_path: Path, min_silence_sec: float = 0.3) -> list:
    """
    æ‰¾åˆ°éŸ³é¢‘ä¸­çš„é™éŸ³ç‚¹ï¼Œç”¨äºåˆ‡åˆ†
    è¿”å›: [(start, end), ...] é™éŸ³åŒºé—´åˆ—è¡¨
    """
    if not HAS_AUDIO_LIBS:
        return []
    
    y, sr = librosa.load(str(audio_path), sr=None)
    
    # è®¡ç®—çŸ­æ—¶èƒ½é‡
    frame_length = int(sr * 0.025)  # 25ms
    hop_length = int(sr * 0.010)    # 10ms
    
    # ä½¿ç”¨ RMS èƒ½é‡
    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
    
    # å½’ä¸€åŒ–
    rms_normalized = rms / (np.max(rms) + 1e-10)
    
    # æ‰¾é™éŸ³å¸§
    is_silence = rms_normalized < SILENCE_THRESHOLD
    
    # è½¬æ¢ä¸ºæ—¶é—´
    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=hop_length)
    
    # æ‰¾è¿ç»­é™éŸ³åŒºé—´
    silence_regions = []
    in_silence = False
    silence_start = 0
    
    for i, silent in enumerate(is_silence):
        if silent and not in_silence:
            in_silence = True
            silence_start = times[i]
        elif not silent and in_silence:
            in_silence = False
            silence_end = times[i]
            if silence_end - silence_start >= min_silence_sec:
                silence_regions.append((silence_start, silence_end))
    
    # å¤„ç†ç»“å°¾é™éŸ³
    if in_silence:
        silence_end = times[-1]
        if silence_end - silence_start >= min_silence_sec:
            silence_regions.append((silence_start, silence_end))
    
    return silence_regions


def split_audio_by_duration(audio_path: Path, max_duration: float = MAX_AUDIO_DURATION) -> list:
    """
    æŒ‰æ—¶é•¿åˆ‡åˆ†éŸ³é¢‘ï¼Œåœ¨é™éŸ³ç‚¹åˆ‡åˆ†
    è¿”å›: [(chunk_path, start_time, end_time), ...]
    """
    if not HAS_AUDIO_LIBS:
        return [(audio_path, 0.0, 0.0)]
    
    y, sr = librosa.load(str(audio_path), sr=None)
    total_duration = len(y) / sr
    
    if total_duration <= max_duration:
        # ä¸éœ€è¦åˆ‡åˆ†
        return [(audio_path, 0.0, total_duration)]
    
    print(f"ğŸ“ éŸ³é¢‘æ—¶é•¿ {total_duration:.1f}s è¶…è¿‡é™åˆ¶ {max_duration}sï¼Œæ­£åœ¨åˆ†æ®µ...")
    
    # æ‰¾é™éŸ³ç‚¹
    silence_points = find_silence_points(audio_path)
    
    # é€‰æ‹©åˆé€‚çš„åˆ‡åˆ†ç‚¹
    chunks = []
    current_start = 0.0
    temp_dir = tempfile.mkdtemp(prefix="forced_align_")
    
    while current_start < total_duration:
        target_end = current_start + max_duration
        
        if target_end >= total_duration:
            # æœ€åä¸€æ®µ
            chunk_end = total_duration
        else:
            # åœ¨ç›®æ ‡ç‚¹é™„è¿‘æ‰¾ä¸€ä¸ªé™éŸ³ç‚¹
            best_split = target_end
            best_distance = float('inf')
            
            for silence_start, silence_end in silence_points:
                # é™éŸ³ä¸­ç‚¹
                silence_mid = (silence_start + silence_end) / 2
                
                # åªè€ƒè™‘åœ¨ç›®æ ‡é™„è¿‘çš„é™éŸ³ç‚¹ï¼ˆå‰å30ç§’èŒƒå›´ï¼‰
                if current_start + 60 < silence_mid < target_end + 30:
                    distance = abs(silence_mid - target_end)
                    if distance < best_distance:
                        best_distance = distance
                        best_split = silence_mid
            
            chunk_end = best_split
        
        # æå–éŸ³é¢‘ç‰‡æ®µ
        start_sample = int(current_start * sr)
        end_sample = int(chunk_end * sr)
        chunk_audio = y[start_sample:end_sample]
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        chunk_path = Path(temp_dir) / f"chunk_{len(chunks):03d}.wav"
        sf.write(str(chunk_path), chunk_audio, sr)
        
        chunks.append((chunk_path, current_start, chunk_end))
        print(f"  ğŸ“¦ ç‰‡æ®µ {len(chunks)}: {current_start:.1f}s - {chunk_end:.1f}s ({chunk_end - current_start:.1f}s)")
        
        current_start = chunk_end
    
    print(f"âœ… å…±åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
    return chunks


def estimate_text_split_position(text: str, audio_chunks: list, total_duration: float) -> list:
    """
    æ ¹æ®éŸ³é¢‘åˆ‡åˆ†ç‚¹ä¼°ç®—æ–‡æœ¬åˆ‡åˆ†ä½ç½®
    è¿”å›: [text_chunk1, text_chunk2, ...]
    """
    if len(audio_chunks) == 1:
        return [text]
    
    # æŒ‰æ—¶é—´æ¯”ä¾‹ä¼°ç®—å­—ç¬¦ä½ç½®
    text_chunks = []
    total_chars = len(text)
    
    for i, (_, start_time, end_time) in enumerate(audio_chunks):
        # è®¡ç®—è¿™æ®µå¯¹åº”çš„å­—ç¬¦èŒƒå›´
        start_ratio = start_time / total_duration
        end_ratio = end_time / total_duration
        
        start_char = int(start_ratio * total_chars)
        end_char = int(end_ratio * total_chars)
        
        # è°ƒæ•´åˆ°å¥å­è¾¹ç•Œï¼ˆå‘å‰æ‰¾æ ‡ç‚¹ï¼‰
        if i > 0 and start_char > 0:
            # å¾€å‰æ‰¾æœ€è¿‘çš„å¥å·/é€—å·
            search_start = max(0, start_char - 50)
            for j in range(start_char, search_start, -1):
                if text[j] in 'ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š':
                    start_char = j + 1
                    break
        
        if i < len(audio_chunks) - 1 and end_char < total_chars:
            # å¾€åæ‰¾æœ€è¿‘çš„å¥å·/é€—å·
            search_end = min(total_chars, end_char + 50)
            for j in range(end_char, search_end):
                if text[j] in 'ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š':
                    end_char = j + 1
                    break
        
        text_chunk = text[start_char:end_char].strip()
        text_chunks.append(text_chunk)
        print(f"  ğŸ“ æ–‡æœ¬ç‰‡æ®µ {i+1}: {len(text_chunk)} å­—ç¬¦")
    
    return text_chunks


def forced_align(script_id: str, podcast_mode: bool = False, use_cpu: bool = False) -> bool:
    """
    ä½¿ç”¨ Qwen3-ForcedAligner ç”Ÿæˆè¯çº§æ—¶é—´æˆ³
    æ”¯æŒé•¿éŸ³é¢‘è‡ªåŠ¨åˆ†æ®µå¤„ç†
    
    Args:
        script_id: é¡¹ç›®æ ‡è¯†ç¬¦
        podcast_mode: æ˜¯å¦ä¸ºæ’­å®¢æ¨¡å¼ï¼ˆä½¿ç”¨æ’­å®¢éŸ³é¢‘å’Œå­—å¹•æ–‡æœ¬ï¼‰
        use_cpu: æ˜¯å¦ä½¿ç”¨ CPU æ¨¡å¼
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    if not HAS_ALIGNER:
        print("âŒ qwen-asr æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå¼ºåˆ¶å¯¹é½")
        return False
    
    paths = get_script_paths(script_id)
    
    if podcast_mode:
        # æ’­å®¢æ¨¡å¼ï¼šä½¿ç”¨æ’­å®¢éŸ³é¢‘ + å­—å¹•æ–‡æœ¬
        audio_path = paths["audio_podcast"]
        text_path = paths["copy_subtitle"]
        print("ğŸ™ï¸ æ’­å®¢æ¨¡å¼ï¼šä½¿ç”¨æ’­å®¢éŸ³é¢‘è¿›è¡Œå¯¹é½")
    else:
        # å¸¸è§„æ¨¡å¼ï¼šäºŒåˆ›éŸ³é¢‘ + äºŒåˆ›æ–‡æ¡ˆ
        audio_path = paths["audio_tts"]
        text_path = paths["copy_recreated"]
    
    # è¾“å‡ºï¼šè¯çº§æ—¶é—´æˆ³
    output_path = paths["word_timestamps"]
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not audio_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        print(f"ğŸ’¡ è¯·å…ˆå‡†å¤‡éŸ³é¢‘æ–‡ä»¶")
        return False
    
    if not text_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡æœ¬æ–‡ä»¶: {text_path}")
        print(f"ğŸ’¡ è¯·å…ˆå‡†å¤‡æ–‡æœ¬æ–‡ä»¶")
        return False
    
    # è¯»å–æ–‡æ¡ˆ
    with open(text_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # è·å–éŸ³é¢‘æ—¶é•¿
    total_duration = get_audio_duration(audio_path)
    
    print(f"ğŸ¯ æ­£åœ¨ä½¿ç”¨ Qwen3-ForcedAligner è¿›è¡Œå¼ºåˆ¶å¯¹é½...")
    print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    print(f"â±ï¸ éŸ³é¢‘æ—¶é•¿: {total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)")
    print(f"ğŸ“„ æ–‡æ¡ˆæ–‡ä»¶: {text_path}")
    print(f"ğŸ“Š æ–‡æ¡ˆå­—æ•°: {len(text)}")
    
    try:
        import torch
        # æ¸…ç©º GPU ç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
        audio_chunks = split_audio_by_duration(audio_path, MAX_AUDIO_DURATION)
        needs_chunking = len(audio_chunks) > 1
        
        if needs_chunking:
            text_chunks = estimate_text_split_position(text, audio_chunks, total_duration)
        else:
            text_chunks = [text]
        
        # åˆå§‹åŒ–æ¨¡å‹
        print(f"\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        if use_cpu:
            print("âš ï¸ ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
            aligner = Qwen3ForcedAligner.from_pretrained(
                "Qwen/Qwen3-ForcedAligner-0.6B",
                device_map="cpu"
            )
            device_info = "cpu"
        else:
            aligner = Qwen3ForcedAligner.from_pretrained(
                "Qwen/Qwen3-ForcedAligner-0.6B",
                device_map="auto",
                dtype=torch.float16
            )
            device_info = "cuda (FP16)"
        
        print(f"ğŸ”§ æ¨¡å‹è®¾å¤‡: {device_info}")
        
        # å¤„ç†æ¯ä¸ªç‰‡æ®µ
        all_words = []
        
        for i, ((chunk_path, chunk_start, chunk_end), text_chunk) in enumerate(zip(audio_chunks, text_chunks)):
            if needs_chunking:
                print(f"\nğŸ“¦ å¤„ç†ç‰‡æ®µ {i+1}/{len(audio_chunks)}...")
            
            try:
                # æ‰§è¡Œå¼ºåˆ¶å¯¹é½
                result = aligner.align(
                    audio=str(chunk_path),
                    text=text_chunk,
                    language="zh"
                )
                
                # æå–è¯çº§æ—¶é—´æˆ³ï¼Œè°ƒæ•´æ—¶é—´åç§»
                # align() è¿”å›æ ¼å¼: [ForcedAlignResult(items=[ForcedAlignItem(...), ...])]
                # å³ï¼šä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«ä¸€ä¸ª ForcedAlignResult å¯¹è±¡
                words = []
                if isinstance(result, list) and len(result) > 0:
                    first_item = result[0]
                    if hasattr(first_item, 'items'):
                        # result = [ForcedAlignResult(items=[...])]
                        words = first_item.items
                    else:
                        # result = [word1, word2, ...] ç›´æ¥æ˜¯è¯åˆ—è¡¨
                        words = result
                elif hasattr(result, 'items'):
                    # result = ForcedAlignResult(items=[...])
                    words = result.items
                elif isinstance(result, dict):
                    words = result.get("words", result.get("items", []))
                else:
                    print(f"âš ï¸ æœªçŸ¥è¿”å›ç±»å‹: {type(result)}")
                
                for w in words:
                    info = extract_word_info(w)
                    all_words.append({
                        "word": info["word"],
                        "start": info["start"] + chunk_start,
                        "end": info["end"] + chunk_start
                    })
                
                if needs_chunking:
                    print(f"  âœ… ç‰‡æ®µ {i+1} å®Œæˆï¼Œè·å– {len(words)} ä¸ªè¯")
                    
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) and not use_cpu:
                    print(f"âš ï¸ GPU æ˜¾å­˜ä¸è¶³ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° CPU æ¨¡å¼...")
                    # é‡æ–°åŠ è½½ CPU æ¨¡å‹
                    del aligner
                    torch.cuda.empty_cache()
                    aligner = Qwen3ForcedAligner.from_pretrained(
                        "Qwen/Qwen3-ForcedAligner-0.6B",
                        device_map="cpu"
                    )
                    # é‡è¯•å½“å‰ç‰‡æ®µ
                    result = aligner.align(
                        audio=str(chunk_path),
                        text=text_chunk,
                        language="zh"
                    )
                    # åŒæ ·å¤„ç† [ForcedAlignResult(items=[...])] æ ¼å¼
                    words = []
                    if isinstance(result, list) and len(result) > 0:
                        first_item = result[0]
                        if hasattr(first_item, 'items'):
                            words = first_item.items
                        else:
                            words = result
                    elif hasattr(result, 'items'):
                        words = result.items
                    elif isinstance(result, dict):
                        words = result.get("words", result.get("items", []))
                    
                    for w in words:
                        info = extract_word_info(w)
                        all_words.append({
                            "word": info["word"],
                            "start": info["start"] + chunk_start,
                            "end": info["end"] + chunk_start
                        })
                else:
                    raise
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if needs_chunking:
            import shutil
            temp_dir = audio_chunks[0][0].parent
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        # æ„å»ºè¾“å‡ºæ ¼å¼
        output_data = {
            "script_id": script_id,
            "full_text": text,
            "segments": [
                {
                    "id": i,
                    "text": w["word"],
                    "start": w["start"],
                    "end": w["end"]
                }
                for i, w in enumerate(all_words)
            ],
            "total_words": len(all_words),
            "segment_mode": "forced_alignment",
            "chunked": needs_chunking,
            "total_duration": total_duration
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ç»“æœ
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—æ€»æ—¶é•¿
        if all_words:
            final_duration = all_words[-1]["end"]
        else:
            final_duration = 0.0
        
        print(f"\nâœ… å¼ºåˆ¶å¯¹é½å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"ğŸ“Š è¯è¯­æ•°é‡: {len(all_words)}")
        print(f"â±ï¸ æ€»æ—¶é•¿: {final_duration:.2f} ç§’")
        
        if needs_chunking:
            print(f"ğŸ“¦ åˆ†æ®µå¤„ç†: {len(audio_chunks)} ä¸ªç‰‡æ®µ")
        
        print("-" * 40)
        print("é¢„è§ˆå‰10ä¸ªè¯ï¼š")
        for seg in output_data["segments"][:10]:
            print(f"  [{seg['start']:.3f}-{seg['end']:.3f}] \"{seg['text']}\"")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¼ºåˆ¶å¯¹é½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Qwen3-ForcedAligner ç”Ÿæˆè¯çº§æ—¶é—´æˆ³")
    parser.add_argument("script_id", help="é¡¹ç›®æ ‡è¯†ç¬¦ (script_id)")
    parser.add_argument("--podcast", "-p", action="store_true",
                       help="æ’­å®¢æ¨¡å¼ï¼šä½¿ç”¨æ’­å®¢éŸ³é¢‘å’Œå­—å¹•æ–‡æœ¬è¿›è¡Œå¯¹é½")
    parser.add_argument("--cpu", action="store_true",
                       help="ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆGPU æ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ï¼‰")

    args = parser.parse_args()

    success = forced_align(args.script_id, podcast_mode=args.podcast, use_cpu=args.cpu)
    sys.exit(0 if success else 1)
